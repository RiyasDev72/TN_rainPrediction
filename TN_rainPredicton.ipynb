{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXXrcHSFTpCp"
   },
   "source": [
    "Web scripting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EC4bllaR7rKh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from existing file with 1891488 rows...\n",
      "Skipping Ariyalur - data already up to date (last date: 2025-09-05)\n",
      "Skipping Chennai - data already up to date (last date: 2025-09-05)\n",
      "Skipping Coimbatore - data already up to date (last date: 2025-09-05)\n",
      "Skipping Cuddalore - data already up to date (last date: 2025-09-05)\n",
      "Skipping Dharmapuri - data already up to date (last date: 2025-09-05)\n",
      "Skipping Dindigul - data already up to date (last date: 2025-09-05)\n",
      "Skipping Erode - data already up to date (last date: 2025-09-05)\n",
      "Skipping Kallakurichi - data already up to date (last date: 2025-09-05)\n",
      "Skipping Kanchipuram - data already up to date (last date: 2025-09-05)\n",
      "Skipping Kanyakumari - data already up to date (last date: 2025-09-05)\n",
      "Skipping Karur - data already up to date (last date: 2025-09-05)\n",
      "Skipping Krishnagiri - data already up to date (last date: 2025-09-05)\n",
      "Skipping Madurai - data already up to date (last date: 2025-09-05)\n",
      "Skipping Nagapattinam - data already up to date (last date: 2025-09-05)\n",
      "Skipping Namakkal - data already up to date (last date: 2025-09-05)\n",
      "Skipping Nilgiris - data already up to date (last date: 2025-09-05)\n",
      "Skipping Perambalur - data already up to date (last date: 2025-09-05)\n",
      "Skipping Pudukkottai - data already up to date (last date: 2025-09-05)\n",
      "Skipping Ramanathapuram - data already up to date (last date: 2025-09-05)\n",
      "Skipping Ranipet - data already up to date (last date: 2025-09-05)\n",
      "Skipping Salem - data already up to date (last date: 2025-09-05)\n",
      "Skipping Sivaganga - data already up to date (last date: 2025-09-05)\n",
      "Skipping Tenkasi - data already up to date (last date: 2025-09-05)\n",
      "Skipping Thanjavur - data already up to date (last date: 2025-09-05)\n",
      "Skipping Theni - data already up to date (last date: 2025-09-05)\n",
      "Skipping Thoothukudi - data already up to date (last date: 2025-09-05)\n",
      "Skipping Tiruchirappalli - data already up to date (last date: 2025-09-05)\n",
      "Skipping Tirunelveli - data already up to date (last date: 2025-09-05)\n",
      "Skipping Tirupathur - data already up to date (last date: 2025-09-05)\n",
      "Skipping Tiruppur - data already up to date (last date: 2025-09-05)\n",
      "Skipping Tiruvallur - data already up to date (last date: 2025-09-05)\n",
      "Skipping Tiruvannamalai - data already up to date (last date: 2025-09-05)\n",
      "Skipping Tiruvarur - data already up to date (last date: 2025-09-05)\n",
      "Skipping Vellore - data already up to date (last date: 2025-09-05)\n",
      "Skipping Viluppuram - data already up to date (last date: 2025-09-05)\n",
      "Skipping Virudhunagar - data already up to date (last date: 2025-09-05)\n",
      "Skipping Chengalpattu - data already up to date (last date: 2025-09-05)\n",
      "Skipping Mayiladuthurai - data already up to date (last date: 2025-09-05)\n",
      "Skipping Ariyalur - data already up to date (last date: 2025-09-05)\n",
      "Skipping Chennai - data already up to date (last date: 2025-09-05)\n",
      "Skipping Coimbatore - data already up to date (last date: 2025-09-05)\n",
      "Skipping Cuddalore - data already up to date (last date: 2025-09-05)\n",
      "Skipping Dharmapuri - data already up to date (last date: 2025-09-05)\n",
      "Skipping Dindigul - data already up to date (last date: 2025-09-05)\n",
      "Skipping Erode - data already up to date (last date: 2025-09-05)\n",
      "Skipping Kallakurichi - data already up to date (last date: 2025-09-05)\n",
      "Skipping Kanchipuram - data already up to date (last date: 2025-09-05)\n",
      "Skipping Kanyakumari - data already up to date (last date: 2025-09-05)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 208\u001b[0m\n\u001b[0;32m    205\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1.5\u001b[39m)\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 208\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[1], line 155\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    152\u001b[0m city_name, lat, lon \u001b[38;5;241m=\u001b[39m TAMIL_NADU_CITIES[city_index]\n\u001b[0;32m    154\u001b[0m \u001b[38;5;66;03m# Get the last date we have data for this city\u001b[39;00m\n\u001b[1;32m--> 155\u001b[0m last_date \u001b[38;5;241m=\u001b[39m get_last_date_for_city(city_name, existing_df)\n\u001b[0;32m    156\u001b[0m current_date \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mdate()\n\u001b[0;32m    158\u001b[0m \u001b[38;5;66;03m# If we already have data up to today, skip this city\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 110\u001b[0m, in \u001b[0;36mget_last_date_for_city\u001b[1;34m(city_name, existing_df)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m existing_df\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m datetime(\u001b[38;5;241m2020\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdate()\n\u001b[1;32m--> 110\u001b[0m city_data \u001b[38;5;241m=\u001b[39m existing_df[existing_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcity\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m city_name]\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m city_data\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m datetime(\u001b[38;5;241m2020\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdate()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m method(\u001b[38;5;28mself\u001b[39m, other)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\arraylike.py:40\u001b[0m, in \u001b[0;36mOpsMixin.__eq__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__eq__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cmp_method(other, operator\u001b[38;5;241m.\u001b[39meq)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:6119\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   6116\u001b[0m lvalues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m   6117\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m extract_array(other, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m-> 6119\u001b[0m res_values \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mcomparison_op(lvalues, rvalues, op)\n\u001b[0;32m   6121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(res_values, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:344\u001b[0m, in \u001b[0;36mcomparison_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invalid_comparison(lvalues, rvalues, op)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m lvalues\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rvalues, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 344\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m comp_method_OBJECT_ARRAY(op, lvalues, rvalues)\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    347\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m _na_arithmetic_op(lvalues, rvalues, op, is_cmp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:130\u001b[0m, in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[1;34m(op, x, y)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     result \u001b[38;5;241m=\u001b[39m libops\u001b[38;5;241m.\u001b[39mscalar_compare(x\u001b[38;5;241m.\u001b[39mravel(), y, op)\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Configuration\n",
    "TARGET_ROWS = 10000000\n",
    "CSV_FILENAME = \"tamil_nadu_rain_prediction_rows.csv\"\n",
    "SELECTED_FEATURES = [\n",
    "    \"temperature_2m\", \"relative_humidity_2m\", \"dew_point_2m\", \"precipitation\",\n",
    "    \"rain\", \"surface_pressure\", \"cloud_cover\", \"cloud_cover_low\",\n",
    "    \"wind_speed_10m\", \"wind_direction_10m\"\n",
    "]\n",
    "\n",
    "TAMIL_NADU_CITIES = [\n",
    "    (\"Ariyalur\", 11.1375, 79.0758),\n",
    "    (\"Chennai\", 13.0827, 80.2707),\n",
    "    (\"Coimbatore\", 11.0168, 76.9558),\n",
    "    (\"Cuddalore\", 11.7447, 79.7680),\n",
    "    (\"Dharmapuri\", 12.1211, 78.1582),\n",
    "    (\"Dindigul\", 10.3621, 77.9765),\n",
    "    (\"Erode\", 11.3410, 77.7172),\n",
    "    (\"Kallakurichi\", 11.7400, 78.9600),\n",
    "    (\"Kanchipuram\", 12.8397, 79.7000),\n",
    "    (\"Kanyakumari\", 8.0883, 77.5385),\n",
    "    (\"Karur\", 10.9574, 78.0809),\n",
    "    (\"Krishnagiri\", 12.5186, 78.2137),\n",
    "    (\"Madurai\", 9.9252, 78.1198),\n",
    "    (\"Nagapattinam\", 10.7667, 79.8417),\n",
    "    (\"Namakkal\", 11.2212, 78.1652),\n",
    "    (\"Nilgiris\", 11.4090, 76.6935),\n",
    "    (\"Perambalur\", 11.2340, 78.8822),\n",
    "    (\"Pudukkottai\", 10.3800, 78.8200),\n",
    "    (\"Ramanathapuram\", 9.3716, 78.8307),\n",
    "    (\"Ranipet\", 12.9254, 79.3323),\n",
    "    (\"Salem\", 11.6643, 78.1460),\n",
    "    (\"Sivaganga\", 9.8432, 78.4809),\n",
    "    (\"Tenkasi\", 8.9601, 77.3153),\n",
    "    (\"Thanjavur\", 10.7869, 79.1378),\n",
    "    (\"Theni\", 10.0104, 77.4768),\n",
    "    (\"Thoothukudi\", 8.7642, 78.1348),\n",
    "    (\"Tiruchirappalli\", 10.7905, 78.7047),\n",
    "    (\"Tirunelveli\", 8.7139, 77.7567),\n",
    "    (\"Tirupathur\", 12.4959, 78.5679),\n",
    "    (\"Tiruppur\", 11.1085, 77.3411),\n",
    "    (\"Tiruvallur\", 13.1449, 79.9087),\n",
    "    (\"Tiruvannamalai\", 12.2262, 79.0746),\n",
    "    (\"Tiruvarur\", 10.7726, 79.6368),\n",
    "    (\"Vellore\", 12.9165, 79.1325),\n",
    "    (\"Viluppuram\", 11.9427, 79.4973),\n",
    "    (\"Virudhunagar\", 9.5827, 77.9807),\n",
    "    (\"Chengalpattu\", 12.6821, 79.9769),\n",
    "    (\"Mayiladuthurai\", 11.1035, 79.6550)\n",
    "]\n",
    "\n",
    "BASE_URL = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "params_template = {\n",
    "    \"hourly\": \",\".join(SELECTED_FEATURES),\n",
    "    \"timezone\": \"Asia/Kolkata\",\n",
    "    \"models\": \"best_match\"\n",
    "}\n",
    "\n",
    "def fetch_weather_data(lat, lon, start_date, end_date):\n",
    "    params = params_template.copy()\n",
    "    params.update({\n",
    "        \"latitude\": lat,\n",
    "        \"longitude\": lon,\n",
    "        \"start_date\": start_date,\n",
    "        \"end_date\": end_date\n",
    "    })\n",
    "    try:\n",
    "        response = requests.get(BASE_URL, params=params, timeout=45)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching data for {lat},{lon}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_and_label_data(data, city_name):\n",
    "    hourly_data = data.get(\"hourly\", {})\n",
    "    df = pd.DataFrame(hourly_data)\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "    df[\"date\"] = df[\"time\"].dt.date\n",
    "    df[\"city\"] = city_name\n",
    "\n",
    "    daily_rain = df.groupby('date')['precipitation'].sum().reset_index()\n",
    "    daily_rain['rain_tomorrow'] = (daily_rain['precipitation'].shift(-1) > 0).astype(int)\n",
    "\n",
    "    df = df.merge(daily_rain[['date', 'rain_tomorrow']], on='date', how='left')\n",
    "    df.drop(columns=['date'], inplace=True)\n",
    "\n",
    "    for col in SELECTED_FEATURES:\n",
    "        if col not in df.columns:\n",
    "            df[col] = np.nan\n",
    "\n",
    "    df = df[SELECTED_FEATURES + ['time', 'city', 'rain_tomorrow']]\n",
    "    df = df.dropna(subset=['rain_tomorrow'])\n",
    "    return df\n",
    "\n",
    "def get_last_date_for_city(city_name, existing_df):\n",
    "    \"\"\"Get the last available date for a specific city\"\"\"\n",
    "    if existing_df.empty:\n",
    "        return datetime(2020, 1, 1).date()\n",
    "\n",
    "    city_data = existing_df[existing_df['city'] == city_name]\n",
    "    if city_data.empty:\n",
    "        return datetime(2020, 1, 1).date()\n",
    "\n",
    "    max_date = pd.to_datetime(city_data['time']).max().date()\n",
    "    return max_date + timedelta(days=1)  # Start from next day\n",
    "\n",
    "def check_duplicates(existing_df, new_df):\n",
    "    \"\"\"Check for duplicates between existing and new data\"\"\"\n",
    "    if existing_df.empty or new_df.empty:\n",
    "        return new_df\n",
    "\n",
    "    # Create unique identifiers for both dataframes\n",
    "    existing_df['unique_id'] = existing_df['time'].astype(str) + '_' + existing_df['city']\n",
    "    new_df['unique_id'] = new_df['time'].astype(str) + '_' + new_df['city']\n",
    "\n",
    "    # Filter out duplicates\n",
    "    mask = ~new_df['unique_id'].isin(existing_df['unique_id'])\n",
    "    result = new_df[mask].copy()\n",
    "\n",
    "    # Clean up temporary columns\n",
    "    result.drop(columns=['unique_id'], inplace=True)\n",
    "    existing_df.drop(columns=['unique_id'], inplace=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "def main():\n",
    "    # Load existing data or create empty dataframe\n",
    "    if os.path.exists(CSV_FILENAME):\n",
    "        existing_df = pd.read_csv(CSV_FILENAME)\n",
    "        existing_df['time'] = pd.to_datetime(existing_df['time'])\n",
    "        total_rows_written = len(existing_df)\n",
    "        print(f\"Resuming from existing file with {total_rows_written} rows...\")\n",
    "    else:\n",
    "        existing_df = pd.DataFrame()\n",
    "        total_rows_written = 0\n",
    "        print(\"Starting new data collection...\")\n",
    "\n",
    "    # Process each city in order\n",
    "    city_index = 0\n",
    "\n",
    "    while total_rows_written < TARGET_ROWS:\n",
    "        city_name, lat, lon = TAMIL_NADU_CITIES[city_index]\n",
    "\n",
    "        # Get the last date we have data for this city\n",
    "        last_date = get_last_date_for_city(city_name, existing_df)\n",
    "        current_date = datetime.now().date()\n",
    "\n",
    "        # If we already have data up to today, skip this city\n",
    "        if last_date >= current_date:\n",
    "            print(f\"Skipping {city_name} - data already up to date (last date: {last_date})\")\n",
    "            city_index = (city_index + 1) % len(TAMIL_NADU_CITIES)\n",
    "            continue\n",
    "\n",
    "        # Set date range (max 30 days per API call)\n",
    "        start_date = last_date\n",
    "        end_date = min(start_date + timedelta(days=30), current_date)\n",
    "\n",
    "        print(f\"Fetching data for {city_name} from {start_date} to {end_date}\")\n",
    "        data = fetch_weather_data(lat, lon, str(start_date), str(end_date))\n",
    "\n",
    "        if data is None:\n",
    "            print(f\"API call failed for {city_name}. Moving to next city...\")\n",
    "            city_index = (city_index + 1) % len(TAMIL_NADU_CITIES)\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "\n",
    "        df_chunk = process_and_label_data(data, city_name)\n",
    "        if df_chunk.empty:\n",
    "            print(f\"No data retrieved for {city_name} in this period. Moving to next city...\")\n",
    "            city_index = (city_index + 1) % len(TAMIL_NADU_CITIES)\n",
    "            time.sleep(1)\n",
    "            continue\n",
    "\n",
    "        # Check for duplicates using the fixed function\n",
    "        df_chunk = check_duplicates(existing_df, df_chunk)\n",
    "\n",
    "        if df_chunk.empty:\n",
    "            print(f\"All fetched data for {city_name} are duplicates. Moving to next city...\")\n",
    "            city_index = (city_index + 1) % len(TAMIL_NADU_CITIES)\n",
    "            continue\n",
    "\n",
    "        # Append to CSV\n",
    "        write_header = not os.path.exists(CSV_FILENAME)\n",
    "        df_chunk.to_csv(CSV_FILENAME, mode='a', header=write_header, index=False)\n",
    "\n",
    "        # Update existing_df\n",
    "        existing_df = pd.concat([existing_df, df_chunk], ignore_index=True)\n",
    "\n",
    "        rows_added = len(df_chunk)\n",
    "        total_rows_written += rows_added\n",
    "        print(f\"Added {rows_added} rows for {city_name}. Total: {total_rows_written}/{TARGET_ROWS}\")\n",
    "\n",
    "        # Move to next city\n",
    "        city_index = (city_index + 1) % len(TAMIL_NADU_CITIES)\n",
    "        time.sleep(1.5)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qTWaVSpnNZpj"
   },
   "source": [
    "Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RHa2QIJGNfs2"
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pyarrow\n",
    "\n",
    "#mounting drive\n",
    "csv = pd.read_csv(\"tamil_nadu_rain_prediction_rows.csv\")\n",
    "# Drop NaN\n",
    "csv.dropna(subset=csv.columns, inplace=True)\n",
    "\n",
    "# Drop last 1470 rows due to nan value\n",
    "csv = csv.iloc[:-1470].reset_index(drop=True)\n",
    "\n",
    "#encoding object dt\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "encoded = encoder.fit_transform(csv[[\"city\"]])\n",
    "encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out([\"city\"]))\n",
    "\n",
    "#making numeric column and raw_df\n",
    "numeric_col = csv.select_dtypes(include=['number'])\n",
    "Raw_df = pd.concat([numeric_col,encoded_df ,csv[\"time\"]], axis=1)\n",
    "\n",
    "#saving MinMaxScaler in scaler variable\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Scale all except last column\n",
    "scaled_values = scaler.fit_transform(Raw_df.iloc[:, :-1])\n",
    "scaled_df = pd.DataFrame(scaled_values, columns=Raw_df.columns[:-1])\n",
    "\n",
    "# Add back last column\n",
    "scaled_df[Raw_df.columns[-1]] = Raw_df.iloc[:, -1].values\n",
    "\n",
    "#finding duplicates\n",
    "df_no_duplicates = scaled_df.drop_duplicates()\n",
    "\n",
    "Final_df_preprocessing = df_no_duplicates.reset_index(drop=True)\n",
    "\n",
    "#saving it\n",
    "Final_df_preprocessing.to_parquet(\"TamilNaduWeather_AfterPreprocessing(NoDup,properindex).parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rvXkPLqkMcpK"
   },
   "source": [
    "Model training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bKpA7BfPMuGA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model was better than dumb ai by 9.70%\n",
      "73.75%\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#save dataframe(parquet)\n",
    "df_model = pd.read_parquet(\"TamilNaduWeather_AfterPreprocessing(NoDup,properindex).parquet\")\n",
    "\n",
    "#seperate input and target columns\n",
    "input_col = df_model.drop([\"time\", \"rain_tomorrow\"], axis=1)\n",
    "target_col = df_model[\"rain_tomorrow\"]\n",
    "\n",
    "#seperate input and target columns for train and test\n",
    "input_train, input_test, target_train, target_test = train_test_split(\n",
    "    input_col, target_col, test_size=0.2, random_state=72)\n",
    "\n",
    "#Train model\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(input_train,target_train)\n",
    "\n",
    "#Model prediction\n",
    "prediction_train = model.predict(input_train)\n",
    "prediction_test = model.predict(input_test)\n",
    "\n",
    "#Model prediction accuracy\n",
    "accuracy_score(target_train,prediction_train)\n",
    "accuracy_score(target_test,prediction_test)\n",
    "\n",
    "#Dumb ai\n",
    "majority_class = target_train.mode()[0]\n",
    "\n",
    "#Dumb ai train\n",
    "y_pred_dumb_train = np.full_like(target_train, fill_value=majority_class)\n",
    "accuracy_score(target_train,y_pred_dumb_train)\n",
    "\n",
    "#Dumb ai test\n",
    "y_pred_dumb_test = np.full_like(target_test, fill_value=majority_class)\n",
    "accuracy_score(target_test,y_pred_dumb_test)\n",
    "\n",
    "#How much was Model prediction of test higher than Dumb ai test accuracy\n",
    "How_better = accuracy_score(target_test,prediction_test) -accuracy_score(target_test,y_pred_dumb_test)\n",
    "print(f\"This model was better than dumb ai by {How_better * 100 :.2f}%\")\n",
    "print(f\"{accuracy_score(target_test,prediction_test)* 100 :.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eRjvni3YPXf7"
   },
   "source": [
    "Saving all data and model with joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xqjAioXtPmPE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TN_rainPredicton_data.joblib']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#imports\n",
    "import joblib\n",
    "\n",
    "#making dictionary\n",
    "TN_rainPredicton_data = {\n",
    "\"model\" : model,\n",
    "\"Final_df_preprocessing\" : Final_df_preprocessing}\n",
    "\n",
    "#dumping it in drive\n",
    "joblib.dump(TN_rainPredicton_data,\"TN_rainPredicton_data.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbF8IJjFm3e8"
   },
   "source": [
    "Predicting Real-world data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "FcCtvPrY1NtF"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Tamil Nadu district:  Chennai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rain tomorrow\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "\n",
    "CITY_COORDS = {\n",
    "    \"Ariyalur\": (11.1375, 79.0758),\n",
    "    \"Chengalpattu\": (12.6821, 79.9769),\n",
    "    \"Chennai\": (13.0827, 80.2707),\n",
    "    \"Coimbatore\": (11.0168, 76.9558),\n",
    "    \"Cuddalore\": (11.7447, 79.7680),\n",
    "    \"Dharmapuri\": (12.1211, 78.1582),\n",
    "    \"Dindigul\": (10.3621, 77.9765),\n",
    "    \"Erode\": (11.3410, 77.7172),\n",
    "    \"Kallakurichi\": (11.7400, 78.9600),\n",
    "    \"Kanchipuram\": (12.8397, 79.7000),\n",
    "    \"Kanyakumari\": (8.0883, 77.5385),\n",
    "    \"Karur\": (10.9574, 78.0809),\n",
    "    \"Krishnagiri\": (12.5186, 78.2137),\n",
    "    \"Madurai\": (9.9252, 78.1198),\n",
    "    \"Mayiladuthurai\": (11.1035, 79.6550),\n",
    "    \"Nagapattinam\": (10.7667, 79.8417),\n",
    "    \"Namakkal\": (11.2212, 78.1652),\n",
    "    \"Nilgiris\": (11.4090, 76.6935),\n",
    "    \"Perambalur\": (11.2340, 78.8822),\n",
    "    \"Pudukkottai\": (10.3800, 78.8200),\n",
    "    \"Ramanathapuram\": (9.3716, 78.8307),\n",
    "    \"Ranipet\": (12.9254, 79.3323),\n",
    "    \"Salem\": (11.6643, 78.1460),\n",
    "    \"Sivaganga\": (9.8432, 78.4809),\n",
    "    \"Tenkasi\": (8.9601, 77.3153),\n",
    "    \"Thanjavur\": (10.7869, 79.1378),\n",
    "    \"Theni\": (10.0104, 77.4768),\n",
    "    \"Thoothukudi\": (8.7642, 78.1348),\n",
    "    \"Tiruchirappalli\": (10.7905, 78.7047),\n",
    "    \"Tirunelveli\": (8.7139, 77.7567),\n",
    "    \"Tirupathur\": (12.4959, 78.5679),\n",
    "    \"Tiruppur\": (11.1085, 77.3411),\n",
    "    \"Tiruvallur\": (13.1449, 79.9087),\n",
    "    \"Tiruvannamalai\": (12.2262, 79.0746),\n",
    "    \"Tiruvarur\": (10.7726, 79.6368),\n",
    "    \"Vellore\": (12.9165, 79.1325),\n",
    "    \"Viluppuram\": (11.9427, 79.4973),\n",
    "    \"Virudhunagar\": (9.5827, 77.9807),\n",
    "}\n",
    "\n",
    "def get_weather(city):\n",
    "    if city not in CITY_COORDS:\n",
    "        city = \"Chennai\"\n",
    "        print(\"City not found. Defaulting to Chennai.\")\n",
    "\n",
    "    lat, lon = CITY_COORDS[city]\n",
    "    url = f\"https://api.open-meteo.com/v1/forecast?latitude={lat}&longitude={lon}&current=temperature_2m,relative_humidity_2m,dew_point_2m,precipitation,rain,surface_pressure,cloud_cover,cloud_cover_low,wind_speed_10m,wind_direction_10m&timezone=auto\"\n",
    "\n",
    "    data = requests.get(url).json()['current']\n",
    "    data['city'] = city\n",
    "    return pd.DataFrame([data])\n",
    "\n",
    "def preprocess_data(df):\n",
    "    # Remove unwanted columns\n",
    "    df = df.drop(columns=['time', 'interval'], errors='ignore')\n",
    "\n",
    "    # Exact city list from training dataset (38 cities)\n",
    "    expected_cities = [\n",
    "        \"Ariyalur\",\"Chengalpattu\",\"Chennai\",\"Coimbatore\",\"Cuddalore\",\n",
    "        \"Dharmapuri\",\"Dindigul\",\"Erode\",\"Kallakurichi\",\"Kanchipuram\",\n",
    "        \"Kanyakumari\",\"Karur\",\"Krishnagiri\",\"Madurai\",\"Mayiladuthurai\",\n",
    "        \"Nagapattinam\",\"Namakkal\",\"Nilgiris\",\"Perambalur\",\"Pudukkottai\",\n",
    "        \"Ramanathapuram\",\"Ranipet\",\"Salem\",\"Sivaganga\",\"Tenkasi\",\n",
    "        \"Thanjavur\",\"Theni\",\"Thoothukudi\",\"Tiruchirappalli\",\"Tirunelveli\",\n",
    "        \"Tirupathur\",\"Tiruppur\",\"Tiruvallur\",\"Tiruvannamalai\",\"Tiruvarur\",\n",
    "        \"Vellore\",\"Viluppuram\",\"Virudhunagar\"\n",
    "    ]\n",
    "\n",
    "    # One-hot encode city\n",
    "    encoder = OneHotEncoder(sparse_output=False, categories=[expected_cities], handle_unknown='ignore')\n",
    "    city_encoded = encoder.fit_transform(df[['city']])\n",
    "    city_cols = [f\"city_{c}\" for c in expected_cities]\n",
    "    city_df = pd.DataFrame(city_encoded, columns=city_cols)\n",
    "\n",
    "    # Numeric features\n",
    "    numeric_cols = [\n",
    "        'temperature_2m', 'relative_humidity_2m', 'dew_point_2m',\n",
    "        'precipitation', 'rain', 'surface_pressure', 'cloud_cover',\n",
    "        'cloud_cover_low', 'wind_speed_10m', 'wind_direction_10m'\n",
    "    ]\n",
    "\n",
    "    existing_numeric_cols = [col for col in numeric_cols if col in df.columns]\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_numeric = scaler.fit_transform(df[existing_numeric_cols])\n",
    "    numeric_df = pd.DataFrame(scaled_numeric, columns=existing_numeric_cols)\n",
    "\n",
    "    # Add missing numeric cols with 0\n",
    "    for col in numeric_cols:\n",
    "        if col not in numeric_df.columns:\n",
    "            numeric_df[col] = 0\n",
    "\n",
    "    numeric_df = numeric_df[numeric_cols]  # reorder\n",
    "\n",
    "    # Final dataset in training order\n",
    "    return pd.concat([numeric_df, city_df], axis=1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    city = input(\"Enter Tamil Nadu district: \").strip().title()\n",
    "    weather_df = get_weather(city)\n",
    "    processed_data = preprocess_data(weather_df)\n",
    "\n",
    "    prediction = model.predict(processed_data)\n",
    "    print(\"Rain tomorrow\" if prediction[0] == 1 else \"No rain tomorrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNVCZyyLn+Ej94YoJXWOdXA",
   "gpuType": "T4",
   "mount_file_id": "1HZ62rxWoat5dt_6TxfuGczwQzc2PNYg0",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
